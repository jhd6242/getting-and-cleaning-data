---
title: "Getting and Cleaning Data Notes"
author: "Justin DeCross"
date: "July 9, 2018"
output: html_document
---

# Getting and Cleaning Data Notes
## Week 1
### Raw and Processed Data
#### Definition of Data
 *	Definition of Data: Data are values of qualitative or quantitative variables, belonging to a set of items.
 *	Qualitative: country of origin, sex, treatment
 *	Qualitative: Height, weight, blood pressure

#### Raw verse processed data
 *	Raw Data
     + The original source of the data
     +	Often hard to use for data analyses
     +	Data analysis includes processing
     +	Raw data may only need to be processed once
    
 *	Processed data on split file
     +	Data that is ready for analysis
     +	Processing can include merging, subsetting, transforming, etc.
     +	There may be standards for processing
     +	All steps should be recorded
    
#### An example of a processing pipeline
 * Human genome project at 1st took 10 years and over $1 billion
 * Now that can be done in a week for $10,000
 * Data is becoming cheaper and faster to obtain
 * Sequencing by synthesis: the brightest color in each sequence is assigned to the corresponding letter
	
### Components of Tidy Data
#### The four things you should have
 *	The raw data
 *	A tidy data set
 *	A code book describing each variable and its value in the tidy data set.
 *	An explicit and exact recipe you use to go from 1-> 2, 3.

#### The raw data
 *	The strange binary file your measurement machine
 *	The unformatted Excel file with 10 worksheets the company you contracted with sent you
 *	The uncomplicated JSON data you got from scraping the Twitter API
 *	The hand-entered numbers you collected looking through a microscope

#### You know the raw data is in the right format if you

 *	Ran no software on the data
 *	Did not manipulate any of the numbers in the data
 *	You did not remove any data from the data set
 *	You did not summarize the data in any way
 
### The tidy data

 *	Each variable you measure should be in one column
 *	Each different observation of that variable should be in a different row
 *	There should be one table for each “kind” of variable
 *  If you have multiple tables, they should include a column in the table that allows them to be linked
 
#### Some other important tips

 *	Include a row at the top of each file with variable names.
 *	Make variable names human readable Age at Diagnosis instead of AgeDx
 *	In general data should be saved in one file per table.
 
### The code book

 *	Information about the variables (including units!) In the data set not contained in the tidy data
 *	Information about summary choices you made
 *	Information about the experimental study design you used
 
#### Some other important tips

 *	A common format is a word/text file.
 *	There should be a section called “study design” that has a thorough description of how you collected the data.
 *	There must be a section called “Code book” that describes each variable and its units.
 
### The instruction list

 *	Ideally a computer script (in R :-)), but I suppose Python is okay too…)
 *	The input for the script in the raw data
 *	The output a process, tidy data
 *	There are no parameters to the scripts
 
#### In some cases it will not be possible to scripts every step. In that case you should provide instruction like:

 *	Step 1 – take the raw file, run version 3.1.2 of summarize software with parameters a = 1, b = 2, c = 3
 *	Step 2 – run the software separately for each sample
 *	Step 3 – take column three of outputfile.out for each sample in this is the corresponding row in the output data set
 
#### Why is the instruction list important?

 *	Faulty data led to many politicians making decisions that were not based or backed up by facts
 
### Downloading Files
#### Get/set your working directory

 *	A basic component of working with data is knowing your working directory
 *	The two main commands are getwd() and setwd()
 *	Be aware of relative verse absolute paths
 *	Relative – setwd(“./data”), setwd(“../”) 
 *	Absolute – setwd(“/Users/jtleek/data/”)
 *	Important differences in Windows setwd(“C:\\Users\\Andrew\\Downloads”)
 
#### Checking for and creating directories

 *	file.exists(“directoryName”) will check to see if the directory exists
 *	dir.create(“directoryName”) will create a directory if it doesn’t exist
 *	here is an example checking for a “data” directory and creating it if it doesn’t exist
 
#### if (!file.exists("data")) {
####   dir.create("data")
#### }

#### Getting data from the Internet-download.file()

 *	Download a file from the Internet
 *	Even if you could do this by hand, helps with reproducibility
 *	Important parameters are url, destfile, method
 *	Useful for downloading tab-delimited, csv, and other files
 
#### Example – Baltimore camera data

 *	public Baltimore camera data
 *	shows you where to download the data
 
#### Download a file from the web
 *	see week 1 downloading a file from the web.R
 
```{r}

#downloading files
#Downloading a file from the web
file_URL <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
download.file( file_URL, destfile = "~/COURSERA/project_1/week 1/cameras.csv")
list.files("C:/Users/wheel/OneDrive/Documents/Justin/Justin/Documents/COURSERA/gettingandcleaning/project/week 1")

date_Downloaded <- date()
date_Downloaded

```
 
#### Some notes about download.file()

 *	if the URL starts with http you can use download.file()
 *	if the URL starts with https on Windows you may be okay
 *	if the URL starts with https on Mac you may need to set method= “curl”
 *	if the file is big, this might take a while
 *	Be sure to record when you download
 
### Reading local flat files


#### Example – Baltimore camera data
#### Download the file to load

 *	See week 1 reading local files.R
 
#### Loading flat files – read.table()

 *	This is the main function for reading data into R
 *	Flexible and robust but requires more parameters
 *	Reads the data into RAM – big data can cause problems
 *	Important parameters file, header, sep, row.names, nrows
 *	Related: read.csv(), read.csv2()
 
#### Baltimore example
 *	See week 1 reading local files.R
 
```{r}
#reading local files
#download the file to load

if (!file.exists("~/COURSERA/project_1/week 1")) {
  dir.create("~/COURSERA/project_1/week 1")
}
file_URL <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
download.file( file_URL, destfile = "~/COURSERA/project_1/week 1/cameras.csv")
date_Downloaded <- date()
date_Downloaded
# Baltimore example
# The wrong way
# camera_Data <- read.table("~/COURSERA/project_1/week 1/cameras.csv")
# head(camera_Data)
# Example: Baltimore camera data
# The right way
camera_Data <- read.table("~/COURSERA/project_1/week 1/cameras.csv",sep = ",", header = TRUE)
head(camera_Data)
#CSV
camera_Data <- read.csv("~/COURSERA/project_1/week 1/cameras.csv")
head(camera_Data)
```
 
#### Example: Baltimore camera data

 *	See week 1 reading local files.R–See the above code
 *	Part 2 CSV: See week 1 reading local files.R
 
#### Some more important parameters


 *	quote – you can tell R whether there are any quoted values quote = “” means no quotes.
 *	na.strings – that the character that represents a missing value
 *	nrows – how many rows to read of the file (e.g. nrows = 10 reads reads 10 lines) 
 *	skip – number of lines to skip before starting to read
 
#### In my experience, the biggest trouble with reading flat files are quotation marks ‘ or ” placed in data values, setting quote = “” often resolve these.

### Reading Excel files

#### Excel files
 *	Still probably the most widely used format for sharing data
 
 
#### Example – Baltimore camera data
 *	Using the Baltimore speeding cameras for another example
 *	There is no Excel file in the Baltimore camera data
 
#### Download the file to load
 *	see week 1 reading Excel files.R
 
#### read.xlsx(), read.xlsx2() {xlsx package}
 *	see week 1 reading Excel files.R
 *	There is no Excel file in the Baltimore camera data

### Reading XML
#### XML

 * Extensible markup language
 * Frequently used story structured data
 * Particularly widely used in Internet applications
 * Extracting XML is the basis of most web scrapings
 * Components
    + Markup – labels give the text structure
    + Content – the actual text of the document
    
#### Tags, elements and attributes

#### Tags corresponding to general label
 * Start tags <section>
 * End tags </section>
 * Empty tags <line-break/>

#### Elements are specific examples of tags

 * <Greeting> Hello, world </Greeting>

#### Attributes are components of the label

 * 
 * <step number= "3"> Connect A to B. </step>

#### Example XML file

 * http://www.w3schools.com/xml/simple.xml

#### Read the file into R


```{r}

library(XML)
	file_URL <- "https://www.w3schools.com/xml/simple.xml"
	download.file(file_URL, "~/COURSERA/project_1/week 1/simple.xml")
	doc  <- xmlTreeParse("~/COURSERA/project_1/week 1/simple.xml",useInternal=TRUE)
	root_Node <- xmlRoot(doc)
	xmlName(root_Node)

	names(root_Node)

	
```

#### Directly access parts of the XML document

```{r}

root_Node[[1]]

root_Node[[1]][[1]]

```

#### Programmatically extract parts of the file


```{r}
xmlSApply(root_Node,xmlValue)
```

#### XPath

 * /node Top level node
 * //node at any level
 * node[@attr-name] Node with any attribute name
 * node [@attr-name='bob'] Node with attribute name attr-name='bob'
 
#### Get the item on the menu and prices

```{r}

xpathSApply(root_Node,"//name",xmlValue)

xpathSApply(root_Node,"//price",xmlValue)

```

#### Another example
 * http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens
 
#### Viewing the source
 * right-click and view source (http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens)

#### Extract content by attributes

```{r}
library(XML)

# Code that works for 2017 Ravens webpage
file_URL <- "http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(file_URL, useInternal=TRUE)
scores <- xpathSApply(doc,"//div[@class='score']",xmlValue)
teams <- xpathSApply(doc,"//div [@class='game-info']",xmlValue)
# print the objects

scores

teams



```

```{r}
# how to get the score in the off-season

file_URL <- 'http://www.espn.com/nfl/team/schedule/_/name/bal/year/2017'
doc <- htmlTreeParse(file_URL,useInternal=TRUE)

# Score vector is straightforward, using <li> tag
score <- xpathSApply(doc,"//li[@class='score']",xmlValue)

# Game status is a bit more tricky, because there are 2 different types
# 1st, we combine the 2 classes in an R object
game_Status <- c("//li[@class='game-status loss']","//li[@class='game-status win']")

# 2nd, we use the object as query string in xpathSApply()
result <- xpathSApply(doc, game_Status,xmlValue)

opponent <- xpathSApply(doc,"//li[@class='team-name']",xmlValue)

# 3rd, use a similar technique to extract game dates from the table, ignoring biweekly
days_of_week <- c("//td['Sun,']","//td['Mon,']","//td['Thu,']","//td['Sat,']")
date <- grep("Sun,|Mon,|Thu,|Sat,",xpathSApply(doc, days_of_week,xmlValue), value=TRUE)
# finally, combined into a data frame and print
team <- rep("Baltimore Ravens",length(opponent))
scores_Data <- data.frame(team,date,opponent,result)
scores_Data

```

#### Notes and further resources

 * Official XML tutorials short, long
 * an outstanding guide to the XML package
 
### Reading JSON
#### JSON

 * JavaScript Object Notation
 * Lightweight data storage
 * Common format for data from application programming interfaces (APIs)
 * Similar structure to XML but different syntax/format
Data stored as
 * Numbers(double)
 * Strings(double quoted)
 * Boolean(true or false)
 * Array(ordered, comma separated enclosed in square brackets[])
 * Object(unordered,comma separated collection of key: value pairs in curly brackets {})

#### Reading data from JSON {jsonlite package}

```{r}
library(jsonlite)
json_Data <- fromJSON("https://api.github.com/users/jtleek/repos")
names(json_Data)
```
#### Nested objects in JSON

```{r}
names(json_Data$owner)

json_Data$owner$login

```

#### Writing data frames to JSON

```{r}

my_json <- toJSON(iris, pretty=TRUE)
cat(my_json)

```

#### Convert back to JSON

```{r}
iris_2 <- fromJSON(my_json)

head(iris_2)
```

#### Further resources

 * different resource links
 * r-bloggers
 
### The data.table Package

#### data.table
 * inherits from data.frame()
	+ All functions that accept data.frame() work on data.table
 * Written in C so it is much faster
 * Much, much faster at subsequent, group, and updating
 
#### Create data tables just like data frames

```{r}

library(data.table)
DF  <- data.frame(x=rnorm(9),y=rep(c("a","b","c"), each=3),z=rnorm(9))
head(DF, 3)

DT <- data.table(x=rnorm(9),y=rep(c("a","b","c"), each=3),z=rnorm(9))
head(DT,3)

```

#### See all the data tables in memory

```{r}
tables()
```

#### Subsetting rows

```{r}

DT[2,]

DT[DT$y == "a",]

DT[c(2,3)]
```

#### Subsetting columns!?

```{r}
# doesn't work this way
DT[,c(2,3)]

```

#### Column subsetting in data.table

 * The subsetting function is modified for data.table
 * The argument you pass after the comma is called an "expression"
 * In R and expression is a collection of statements enclosed in curly brackets

```{r}
{
	x = 1
	y = 2
}
k = {print(10); 5}

print(k)
```

#### Calculating values for variables with expressions

```{r}
DT[,list(mean(x),sum(z))]

DT[,table(y)]

```

#### Adding new columns

```{r}

DT[,w:=z^2]
DT

DT2 <- DT
DT[, y:= 2]

```

#### Careful

```{r}
head(DT, n=3)

head(DT2, n=3)
```

#### Multiple operations

```{r}

DT[,m:= {tmp <- (x+z); log2(tmp+5)}]
DT
```

#### plyr like operations

```{r}
DT[,a:=x>0]
DT

DT[,b:=mean(x+w),by =a]
DT
```

#### Special variables
.N an integer, length 1, containing the number

```{r}
set.seed(123);
DT <- data.table(x=sample(letters[1:3],1E5,TRUE))
DT[, .N, by=x ]
```

#### Keys

```{r}
DT <- data.table(x=rep(c("a","b","c"), each= 100),y=rnorm(300))
setkey(DT, x)
DT['a']
```

#### Joins

```{r}
DT1 <- data.table(x=c('a','a','b','dt1'),y=1:4)
DT2 <- data.table(x=c('a','b','dt2'),z=5:7)
setkey(DT1,x);setkey(DT2,x)
merge(DT1, DT2)
```

#### Fast reading

```{r}
big_df <- data.frame(x=rnorm(1E6),y =rnorm(1E6))
file <- tempfile()
write.table(big_df, file=file,row.names=FALSE, col.names=TRUE, sep="\t", quote=FALSE)
system.time(fread(file))

system.time(read.table(file, header=TRUE, sep="\t"))

```

#### Summary and further reading
 * The latest development version contains new functions like melt and dcast
 * Here is a list of differences between data.table and data.frame
 * More notes
 
## Week 2
### Reading from mySQL
#### mySQL

 * Free and widely used open source database software
 * Widely used in internet-based applications
 * Data are structured in
	+ Databases
	+ Tables within databases
	+ Fields within tables
 * Each row is called a record
 
#### Example structure
 * a series of linked tables

#### Step 1 – Install MySQL

 * I just installed the packages I didn't have to do anything else

#### Step 2 – Install RMySQL
 * install.packages("RMySQL") works for my Windows machine I also had to install
DBI package

#### Example – UCSC database

 * http://genome.ucsc.edu/

#### UCSC MySQL

 * http://genome.ucsc.edu/goldenPath/help/mysql.html

#### Connecting and listing databases

```{r}
library("RMySQL")
ucscDb <- dbConnect(MySQL(), user="genome",
				host="genome-mysql.cse.ucsc.edu")
result <- dbGetQuery(ucscDb,"show databases;"); dbDisconnect;
result
```

#### Connecting to hg19 and listing tables

```{r}
hg19 <- dbConnect(MySQL(), user="genome", db="hg19",
				host="genome-mysql.cse.ucsc.edu")
all_Tables <- dbListTables(hg19)
length(all_Tables)

all_Tables[1:5]

```

### Getting dimensions of a specific table

```{r}
dbListFields(hg19,"affyU133Plus2")

dbGetQuery(hg19,"select count(*) from affyU133Plus2")
```

#### Read from the table

```{r}
affyData <- dbReadTable(hg19,"affyU133Plus2")
head(affyData)
```

#### Select a specific subset

```{r}
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis <- fetch(query); quantile(affyMis$misMatches)

affMisSmall <- fetch(query,n=10);dbClearResult(query);

dim(affMisSmall)
```


#### Don't forget to close the connection!

```{r}
dbDisconnect(hg19)
```

#### Further resources
 * only use only use the select commands
 
### Reading HDF5
#### HDF5
 * Used for storing large data sets
 * Supports storing a range of data types
 * Hierarchical data format
 * groups containing 0 or more data sets and metadata
	+ Have a group header with group name and list of attributes
	+ Have a group symbol table with a list of objects in group
 * datasets multidimensional array of data elements with metadata
	+ Have a header with name, data type, data space, and storage layout
	+ Have a data array with the data
	
#### R HDF5 package

```{r}
source("https://bioconductor.org/biocLite.R")
biocLite("rhdf5")

library(rhdf5)
created = h5createFile("example.h5")
created
```

 * This will install packages from bioconductor, primary use for genomics but also have good "big data" packages
 * Can be used to interface with HDF5
 
